{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  { 
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import math\n",
    "import pickle \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from collections import OrderedDict\n",
    "from itertools import islice\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network spam classifier\n",
    "##In order to classify messages as spam or ham, we are going to use a neural network. This model have one input, hidden and output layer, and the sigmoid function as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.weights_input_hidden = np.random.uniform(-1,1,size=(hidden_nodes, input_nodes))\n",
    "        self.weights_hidden_output = np.random.uniform(-1,1,size=(output_nodes, hidden_nodes))\n",
    "        self.bias_hidden = np.ones((hidden_nodes,1))\n",
    "        self.bias_output = np.ones((output_nodes,1))\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+ math.exp(-x))\n",
    "    \n",
    "    def derivate(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "   \n",
    "    def feedforward(self,input_v):\n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        \n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "    \n",
    "        hidden = np.dot(self.weights_input_hidden,input_vector)\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "    \n",
    "        output = np.dot(self.weights_hidden_output, hidden)\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def backpropagation(self, input_v, target_v):\n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "        target_vector = target_v.reshape((self.output_nodes,1))\n",
    "        \n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        derivate_vector = np.vectorize(self.derivate)\n",
    "    \n",
    "        hidden = np.dot(self.weights_input_hidden,input_vector)\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "        \n",
    "        output = np.dot(self.weights_hidden_output, hidden)\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "\n",
    "        \n",
    "\n",
    "        output_error = np.subtract(target_vector,output)\n",
    "        error = output_error.sum(0)\n",
    "        \n",
    "        gradient = derivate_vector(output)\n",
    "        gradient = np.multiply(gradient,output_error)\n",
    "        gradient = np.multiply(gradient, self.learning_rate)\n",
    "        \n",
    "        hidden_transpose = np.transpose(hidden)\n",
    "        weights_ho_deltas = np.dot(gradient, hidden_transpose)\n",
    "        \n",
    "        self.weights_hidden_output = np.add(self.weights_hidden_output, weights_ho_deltas)\n",
    "        self.bias_output = np.add(self.bias_output, gradient)\n",
    "        \n",
    "        \n",
    "        transpose_weights_hidden_output = np.transpose(self.weights_hidden_output)\n",
    "        hidden_error = np.dot(transpose_weights_hidden_output, output_error)\n",
    "        \n",
    "    \n",
    "        hidden_gradient = derivate_vector(hidden)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, hidden_error)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, self.learning_rate)\n",
    "        \n",
    "        input_transpose = np.transpose(input_vector)\n",
    "        weights_ih_deltas = np.dot(hidden_gradient, input_transpose)\n",
    "        \n",
    "        self.weights_input_hidden = np.add(self.weights_input_hidden, weights_ih_deltas)\n",
    "        self.bias_hidden = np.add(self.bias_hidden, hidden_gradient)\n",
    "\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def train(self, train_dataframe, epochs):\n",
    "        spam = 0\n",
    "        ham = 0\n",
    "        iteration = 0\n",
    "        error_sample = 200\n",
    "        errors = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch\", i)\n",
    "            for index, row in train_dataframe.iterrows():\n",
    "                spam+=(row['label_tag'])  \n",
    "                input_v = row.to_numpy()\n",
    "                input_v = input_v[1:len(input_v)-1]\n",
    "                target_v = np.array([row['label_tag']])\n",
    "                error = self.backpropagation(input_v, target_v)\n",
    "                \n",
    "                if iteration%error_sample == 0:\n",
    "                    errors.append(error)\n",
    "                    print(\"Iteration\", iteration, \"error\", error)\n",
    "                iteration += 1\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        ham = (len(train_dataframe)*epochs)-spam\n",
    "        print(f\"Spam:{spam} - Ham:{ham}\")\n",
    "        print(\"Done\")  \n",
    "\n",
    "        return np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are utils class for parsing, reading and saving data from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil:\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(message):\n",
    "        message = re.sub(r\"\\$[\\d]+\",'price',message)\n",
    "        message = re.sub(r\"\\%[\\d]+\",'percentage',message)\n",
    "        message = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'email',message)\n",
    "        message = re.sub(r'[\\W\\s\\d]',' ',message)\n",
    "        return message\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_data(message):\n",
    "        message = message.lower() \n",
    "        message = DataUtil.normalize_data(message)\n",
    "        words = nltk.word_tokenize(message)\n",
    "\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word not in DataUtil.stop_words and len(word)>2:   \n",
    "                #words = DataUtil.stemmer.stem(words[i])\n",
    "                word = DataUtil.lemmatizer.lemmatize(word)\n",
    "                result.append(word)  \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def order_and_take(data, key, n=None):\n",
    "        data = OrderedDict(sorted(data.items(), key=lambda i: i[1][key], reverse=True))\n",
    "        if n!=None:\n",
    "            data = dict(islice(data.items(), n))\n",
    "        return data\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        self.words_data = {}\n",
    "\n",
    "    def get_words(self):\n",
    "        df = pd.read_csv(self.document)\n",
    "        words_list = dict()\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            words = DataUtil.clean_data(row['message'])\n",
    "            for word in words: \n",
    "                if word not in words_list.keys():\n",
    "                    words_list[word] = 1\n",
    "                else:\n",
    "                    words_list[word] += 1\n",
    "        \n",
    "        result = { key:val for key, val in words_list.items() if val > 10}\n",
    "        result = nlargest(3000, result, key=result.get)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    @staticmethod\n",
    "    def tf(sentences):    \n",
    "        words_counter = {}\n",
    "        for index, sentence in enumerate(sentences):\n",
    "            words = DataUtil.clean_data(sentence)\n",
    "            for word in words: \n",
    "                    if word not in words_counter.keys(): \n",
    "                        words_counter[word] = {}\n",
    "                        words_counter[word]['sentences'] = {}\n",
    "                    if index not in words_counter[word]['sentences'].keys():\n",
    "                        words_counter[word]['sentences'][index] = 1/len(words)         \n",
    "                    else:\n",
    "                        words_counter[word]['sentences'][index] += 1/len(words)\n",
    "        return words_counter\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_idf(message):\n",
    "        sentences = nltk.sent_tokenize(message)\n",
    "        words_count = Data.tf(sentences)\n",
    "        words_data = {}\n",
    "\n",
    "        for key, element in words_count.items():\n",
    "            words_data[key] = [0 for i in range(len(sentences))]\n",
    "            idf = math.log(len(sentences)/len(element['sentences']))\n",
    "            for index, sentence_ratio in element['sentences'].items():    \n",
    "                words_data[key][index] = sentence_ratio * idf\n",
    "        return words_data\n",
    "          \n",
    "    @staticmethod\n",
    "    def get_inputs_count(message, words_list):  \n",
    "        words = DataUtil.clean_data(message)  \n",
    "        inputs = np.zeros(len(words_list))\n",
    "\n",
    "        for index, word in enumerate(words_list):\n",
    "            if word in words:\n",
    "                inputs[index] +=1 \n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_unique_words(dataframe):\n",
    "        unique_words = {}\n",
    "        for index,row in dataframe.iterrows():\n",
    "            unique_words[row['word']] = 0\n",
    "        return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all we need to get all the unique words from our dataset, so we parse each message into tokens and keep the 3000 most frequent words"
   ]
  }
 ]
}